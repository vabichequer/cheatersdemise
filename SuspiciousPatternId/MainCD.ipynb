{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheaters demise code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code defined the clusters of suspicious users, which were characterized as those who didn't watch (almost) any videos and documents and got all the answers right. These are thought to be users who deal with fake and real accounts, counting errors on the fakes and taking the right answers on the real ones.\n",
    "\n",
    "Since there is a defined cluster of suspicious users, the following code will try to identify the activity of these users and see if they can be considered as cheaters or not, depending on timing and improvement. By taking into account that timing would be equal to the time difference between an answer from one user and the other, there may be a correlation between them, since after finding the real answer on a fake account, the user would input it into his real account in a short period. Also, improvement is a metric that can differentiate between a user that coincidently did it right after another and a cheater, by measuring if the user got the wrong answer or not. If it did get the wrong answer, than he's not using a fake account.\n",
    "\n",
    "One course of action is to create a graph of exercise (i) by user(j) and the time it took the user to finish it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /lustre/home/vinte/.local/lib/python3.6/site-packages (3.1.2)\n",
      "Requirement already satisfied: scipy in /lustre/home/vinte/.local/lib/python3.6/site-packages (1.4.1)\n",
      "Requirement already satisfied: networkx in /lustre/home/vinte/.local/lib/python3.6/site-packages (2.4)\n",
      "Requirement already satisfied: seaborn in /lustre/home/vinte/.local/lib/python3.6/site-packages (0.10.0)\n",
      "Requirement already satisfied: sklearn in /lustre/home/vinte/.local/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: pandas in /lustre/home/vinte/.local/lib/python3.6/site-packages (0.25.3)\n",
      "Requirement already satisfied: sympy in /lustre/home/vinte/.local/lib/python3.6/site-packages (1.5.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /lustre/home/vinte/.local/lib/python3.6/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /lustre/home/vinte/.local/lib/python3.6/site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.11 in /lustre/local/lib64/python3.6/site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /lustre/home/vinte/.local/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /lustre/home/vinte/.local/lib/python3.6/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /lustre/home/vinte/.local/lib/python3.6/site-packages (from networkx) (4.4.1)\n",
      "Requirement already satisfied: scikit-learn in /lustre/home/vinte/.local/lib/python3.6/site-packages (from sklearn) (0.22.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /lustre/home/vinte/.local/lib/python3.6/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /lustre/home/vinte/.local/lib/python3.6/site-packages (from sympy) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /lustre/local/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (45.1.0)\n",
      "Requirement already satisfied: six in /lustre/local/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /lustre/home/vinte/.local/lib/python3.6/site-packages (from scikit-learn->sklearn) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install matplotlib scipy networkx seaborn sklearn pandas sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Import all definitions and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries and definitions loaded correctly.\n",
      "Libraries and definitions loaded correctly.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import Definitions\n",
    "import ctd\n",
    "\n",
    "importlib.reload(Definitions)\n",
    "importlib.reload(ctd)\n",
    "\n",
    "from Definitions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read scores and user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = readDataFile(\"eventos_final.json\")\n",
    "scores_csv = pd.read_csv(\"UAMx_Android301x_1T2015_grade_report_2015-04-21-1145_sanitized.csv\")\n",
    "ids_sospechosos = pd.read_csv(\"UserIDClusterSospechoso.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinte/.local/lib/python3.6/site-packages/ipykernel_launcher.py:22: FutureWarning: `item` has been deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished logging scores.\n",
      "Logging exercises...\n",
      "Ended logging.\n"
     ]
    }
   ],
   "source": [
    "N_USERS = len(df.Usuario)\n",
    "\n",
    "# create the exercise list\n",
    "correctExercises = np.empty((N_USERS, N_EXERCISES))\n",
    "correctExercises.fill(np.nan)\n",
    "\n",
    "correctExercisesCount = np.empty(N_USERS)\n",
    "correctExercisesCount.fill(0)\n",
    "\n",
    "wrongExercises = np.empty((N_USERS, N_EXERCISES))\n",
    "wrongExercises.fill(np.nan)\n",
    "\n",
    "wrongExercisesCount = np.empty(N_USERS)\n",
    "wrongExercisesCount.fill(0)\n",
    "\n",
    "scores = []\n",
    "\n",
    "print(\"Logging scores...\")\n",
    "\n",
    "for i in range(1, len(df.Usuario)):\n",
    "    try:\n",
    "        score = scores_csv.loc[scores_csv['id'] == int(df.Usuario[i])]['grade'].item()\n",
    "        scores.append((int(df.Usuario[i]), score))\n",
    "    except:                      \n",
    "        scores.append((int(df.Usuario[i]), np.nan))\n",
    "\n",
    "    #clear()\n",
    "    #print('Scores:', math.ceil(i*100/len(df.Usuario)), '% done')\n",
    "\n",
    "print(\"Finished logging scores.\")\n",
    "print(\"Logging exercises...\")\n",
    "\n",
    "for i in range(0, len(df.Usuario)):\n",
    "    for j in range(0, len(df.Eventos[i])):\n",
    "        if (df.Eventos[i][j]['evento'] == 'problem_check'):\n",
    "            #print('\\t problem_check')\n",
    "            if (df.Eventos[i][j]['resultados'] != []):\n",
    "                #print('\\t \\t results')\n",
    "                # convert date time to epoch time for a better comparison\n",
    "                time_split = df.Eventos[i][j]['tiempo'].split('T')\n",
    "                time_tuple = time.strptime(time_split[0] + ' ' + time_split[1][:8], date_format)\n",
    "                time_epoch = time.mktime(time_tuple)   \n",
    "                if(df.Eventos[i][j]['resultados'][0]['correcto'] == 'True'):\n",
    "                    #print('\\t \\t \\t right')\n",
    "                    correctExercises[i][int(df.Eventos[i][j]['id_problema']) - 1] = time_epoch\n",
    "                    correctExercisesCount[i] = correctExercisesCount[i] + 1\n",
    "                elif(df.Eventos[i][j]['resultados'][0]['correcto'] == 'False'):\n",
    "                    #print('\\t \\t \\t wrong')\n",
    "                    wrongExercises[i][int(df.Eventos[i][j]['id_problema']) - 1] = time_epoch\n",
    "                    num_intentos = int(df.Eventos[i][j]['num_intentos'])\n",
    "                    wrongExercisesCount[i] = (wrongExercisesCount[i] - num_intentos + 1) + num_intentos\n",
    "    #clear()\n",
    "    #print('Exercises:', math.ceil(i*100/N_USERS), '% done')\n",
    "\n",
    "print('Ended logging.')\n",
    "\n",
    "N_USERS = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_USERS = len(df.Usuario)\n",
    "\n",
    "correctExercises_minutes = correctExercises / 60\n",
    "wrongExercises_minutes = wrongExercises / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting CC users...\n",
      "Selecting XC users...\n",
      "Selecting CX users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/home/vinte/cheatersdemise/SuspiciousPatternId/Definitions.py:81: RuntimeWarning: invalid value encountered in less_equal\n",
      "  time_dif = time_dif[time_dif <= 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting XX users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/home/vinte/cheatersdemise/SuspiciousPatternId/Definitions.py:83: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  time_dif = time_dif[time_dif >= 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CX thread finished. CX added 25708035 users.\n",
      "XC thread finished. XC added 25708035 users.\n"
     ]
    }
   ],
   "source": [
    "dump_exists = os.path.isfile(str(tol) + '/' + str(tol) + '.csv')\n",
    "\n",
    "if (dump_exists):\n",
    "    df_all_selected_users = pd.read_csv(str(tol) + '/' + str(tol) + '.csv', index_col=0)\n",
    "    df_all_selected_users.fillna('', inplace=True)\n",
    "\n",
    "    selected_users_CC = literal_eval(df_all_selected_users.loc[0][0])\n",
    "    selected_users_XC = literal_eval(df_all_selected_users.loc[1][0])\n",
    "    selected_users_CX = literal_eval(df_all_selected_users.loc[2][0])\n",
    "    selected_users_XX = literal_eval(df_all_selected_users.loc[3][0])\n",
    "    \n",
    "    print('Dump loaded.')\n",
    "else:\n",
    "    #if __name__ == '__main__':\n",
    "        #__spec__ = None\n",
    "        selected_users_CC = Manager().list()\n",
    "        selected_users_XC = Manager().list()\n",
    "        selected_users_CX = Manager().list()\n",
    "        selected_users_XX = Manager().list()\n",
    "\n",
    "        print('Selecting CC users...')\n",
    "        p_CC = Process(target=selectUsers, args=(selected_users_CC, tol, correctExercises_minutes, correctExercises_minutes, 'CC', N_USERS))\n",
    "        p_CC.start()\n",
    "        print('Selecting XC users...')\n",
    "        p_XC = Process(target=selectUsers, args=(selected_users_XC, tol, wrongExercises_minutes, correctExercises_minutes, 'XC', N_USERS))\n",
    "        p_XC.start()\n",
    "        print('Selecting CX users...')\n",
    "        p_CX = Process(target=selectUsers, args=(selected_users_CX, tol, correctExercises_minutes, wrongExercises_minutes, 'CX', N_USERS))\n",
    "        p_CX.start()\n",
    "        print('Selecting XX users...')\n",
    "        p_XX = Process(target=selectUsers, args=(selected_users_XX, tol, wrongExercises_minutes, wrongExercises_minutes, 'XX', N_USERS))\n",
    "        p_XX.start()\n",
    "\n",
    "        p_CC.join()\n",
    "        p_XC.join()\n",
    "        p_CX.join()\n",
    "        p_XX.join()\n",
    "\n",
    "        df_all_selected_users = pd.DataFrame([selected_users_CC, selected_users_XC, selected_users_CX, selected_users_XX])\n",
    "        df_all_selected_users.to_csv(str(tol) + '/' + str(tol) + '.csv')\n",
    "\n",
    "        print(\"Data stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the time difference between the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    __spec__ = None\n",
    "    time_differences_CC = Manager().list()\n",
    "    time_differences_XC = Manager().list()\n",
    "    time_differences_CX = Manager().list()\n",
    "    time_differences_XX = Manager().list()\n",
    "\n",
    "    print('Time differentiating CC users...')\n",
    "    p_CC = Process(target=ctd.computeTimeDifferences, args=(time_differences_CC, selected_users_CC, \n",
    "                                                        correctExercises_minutes, correctExercises_minutes, 'CC', N_EXERCISES,))\n",
    "    p_CC.start()\n",
    "    print('Time differentiating XC users...')\n",
    "    p_XC = Process(target=ctd.computeTimeDifferences, args=(time_differences_XC, selected_users_XC,\n",
    "                                                        wrongExercises_minutes, correctExercises_minutes, 'XC', N_EXERCISES,))\n",
    "    p_XC.start()\n",
    "    print('Time differentiating CX users...')\n",
    "    p_CX = Process(target=ctd.computeTimeDifferences, args=(time_differences_CX, selected_users_CX,\n",
    "                                                        correctExercises_minutes, wrongExercises_minutes, 'CX', N_EXERCISES,))\n",
    "    p_CX.start()\n",
    "    print('Time differentiating XX users...')\n",
    "    p_XX = Process(target=ctd.computeTimeDifferences, args=(time_differences_XX, selected_users_XX,\n",
    "                                                        wrongExercises_minutes, wrongExercises_minutes, 'XX', N_EXERCISES,))\n",
    "    p_XX.start()\n",
    "\n",
    "    p_CC.join()\n",
    "    p_XC.join()\n",
    "    p_CX.join()\n",
    "    p_XX.join()\n",
    "    \n",
    "    print('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users in common\n",
    "\n",
    "Finds which users are considered into the selection, without repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_in_common = []\n",
    "\n",
    "for k in range(0, len(selected_users_CC)):\n",
    "    if (selected_users_CC[k][0] not in users_in_common):\n",
    "        users_in_common.append(selected_users_CC[k][0])\n",
    "    if (selected_users_CC[k][1] not in users_in_common):\n",
    "        users_in_common.append(selected_users_CC[k][1]) \n",
    "\n",
    "for l in range(0, len(selected_users_XC)):\n",
    "    if (selected_users_XC[l][0] not in users_in_common):\n",
    "        users_in_common.append(selected_users_XC[l][0])\n",
    "    if (selected_users_XC[l][1] not in users_in_common):\n",
    "        users_in_common.append(selected_users_XC[l][1])\n",
    "\n",
    "for m in range(0, len(selected_users_CX)):\n",
    "    if (selected_users_CX[m][0] not in users_in_common):\n",
    "        users_in_common.append(selected_users_CX[m][0])\n",
    "    if (selected_users_CX[m][1] not in users_in_common):\n",
    "        users_in_common.append(selected_users_CX[m][1]) \n",
    "\n",
    "for n in range(0, len(selected_users_XX)):\n",
    "    if (selected_users_XX[n][0] not in users_in_common):\n",
    "        users_in_common.append(selected_users_XX[n][0])\n",
    "    if (selected_users_XX[n][1] not in users_in_common):\n",
    "        users_in_common.append(selected_users_XX[n][1])\n",
    "\n",
    "print('\\t Result size:', len(users_in_common))\n",
    "print('\\t Result data:', users_in_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join type arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "all_selected_users = []\n",
    "all_time_differences = []\n",
    "\n",
    "versions = ['CC', 'XC', 'CX', 'XX']\n",
    "temp_users = [list(selected_users_CC), list(selected_users_XC), list(selected_users_CX), list(selected_users_XX)]\n",
    "temp_td = [list(time_differences_CC), list(time_differences_XC), list(time_differences_CX), list(time_differences_XX)]\n",
    "temp_size = [len(selected_users_CC), len(selected_users_XC), len(selected_users_CX), len(selected_users_XX)]\n",
    "\n",
    "#for l in range(0, 4):        \n",
    "#    z = temp_size.index(max(temp_size))\n",
    "#    label.append(versions[z])\n",
    "#    all_selected_users.append(temp_users[z])\n",
    "#    all_time_differences.append(temp_td[z])\n",
    "#    temp_size.pop(z) \n",
    "#    versions.pop(z) \n",
    "#    temp_users.pop(z)\n",
    "#    temp_td.pop(z)\n",
    "\n",
    "all_selected_users = temp_users\n",
    "all_time_differences = temp_td\n",
    "label = versions\n",
    "\n",
    "print(label)\n",
    "print(label[0], len(all_selected_users[0]), '||', len(selected_users_CC))\n",
    "print(label[1], len(all_selected_users[1]), '||', len(selected_users_XC))\n",
    "print(label[2], len(all_selected_users[2]), '||', len(selected_users_CX))\n",
    "print(label[3], len(all_selected_users[3]), '||', len(selected_users_XX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_array = type_separation(all_selected_users, all_time_differences, tol)\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate user bias and pairs through the type arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_pairs, user_score_difference, fig, string_dump, user_interaction_percentages = generate_pairs(type_array, df.Usuario, scores, trimming, label, plot=True)\n",
    "\n",
    "with open('user_bias_dump.txt', 'w') as filehandle:\n",
    "    json.dump(string_dump, filehandle)\n",
    "\n",
    "print('Done.')\n",
    "    \n",
    "print('Plotting', len(user_pairs), 'pairs consisted of', len(users_in_common), 'users')\n",
    "fig.set_size_inches(20, len(fig.axes) * 6)\n",
    "fig.tight_layout()\n",
    "plt.savefig(str(tol) + '/' + str(trimming) + '-user_bias.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process type array and eliminate users without a score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_array = np.array(type_array)\n",
    "user_score_difference = np.array(user_score_difference)\n",
    "\n",
    "total_exercises_under_tol = type_array[:, 3]\n",
    "type_array = np.reshape(type_array[:, 2], (-1, 1))\n",
    "user_score_difference = np.reshape(user_score_difference, (-1, 1))\n",
    "\n",
    "analysing_data = np.concatenate((type_array, user_score_difference), axis=1)\n",
    "\n",
    "analysing_data = np.array(analysing_data, np.float)\n",
    "\n",
    "user_pairs_copy = user_pairs.copy()\n",
    "\n",
    "i = 0\n",
    "while i < len(analysing_data):\n",
    "    if(np.isnan(analysing_data[i,1])):\n",
    "        user_pairs_copy.pop(i)\n",
    "        user_interaction_percentages.pop(i)\n",
    "        user_score_difference = np.delete(user_score_difference, i, 0)\n",
    "        analysing_data = np.delete(analysing_data, i, 0)\n",
    "        total_exercises_under_tol = np.delete(total_exercises_under_tol, i, 0)\n",
    "    else:\n",
    "        i = i + 1\n",
    "\n",
    "print(tol, len(user_pairs_copy), len(users_in_common), 'pairs remaining.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_score_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the amout of exercise by user through the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.xlabel('User bias', fontsize=25)\n",
    "plt.ylabel('Score difference', fontsize=25)\n",
    "plt.title(\"Amount of exercises by user\", fontsize=25)\n",
    "\n",
    "scatter = plt.scatter(analysing_data[:, 0], analysing_data[:, 1], s=250, edgecolors = 'black', c=total_exercises_under_tol, cmap='binary')\n",
    "\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "#for i, txt in enumerate(user_pairs_copy):\n",
    "#    plt.annotate(txt, (analysing_data[i, 0], analysing_data[i, 1]))\n",
    "\n",
    "x = np.linspace(-1, 1, 201)\n",
    "y = [pow(i, 9) for i in x]\n",
    "plt.plot(x, y)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.1, 1.1])\n",
    "axes.set_ylim([-1.1, 1.1])\n",
    "plt.xticks(fontsize=20)\n",
    "\n",
    "plt.grid(color='grey', linestyle='--', linewidth=.5)\n",
    "\n",
    "plt.savefig(str(tol) + '/' + str(trimming) + '-amount_of_exercises.png', bbox_inches='tight')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the distance from optimal curve (X^9) and separate outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.xlabel('User bias')\n",
    "plt.ylabel('Course final score difference between users')\n",
    "plt.title(\"Distance from optimal curve (X^9)\")\n",
    "\n",
    "normal_points = []\n",
    "outliers = []\n",
    "\n",
    "for i, txt in enumerate(analysing_data):\n",
    "    dist = distance(analysing_data[i, 0], analysing_data[i, 1])\n",
    "    if (dist > 0.25):\n",
    "        plt.annotate(round(dist, 2), (analysing_data[i, 0], analysing_data[i, 1])) \n",
    "        outliers.append([analysing_data[i, 0], analysing_data[i, 1], user_pairs_copy[i][0], user_pairs_copy[i][1]])\n",
    "    else:\n",
    "        normal_points.append([analysing_data[i, 0], analysing_data[i, 1], user_pairs_copy[i][0], user_pairs_copy[i][1]])\n",
    "\n",
    "#for i, txt in enumerate(user_pairs_copy):\n",
    "#    plt.annotate(txt, (analysing_data[i, 0], analysing_data[i, 1]))\n",
    "\n",
    "normal_points = np.asarray(normal_points)\n",
    "outliers = np.asarray(outliers)\n",
    "\n",
    "plt.scatter(normal_points[:, 0], normal_points[:, 1], marker='o', color='blue', picker=True)   \n",
    "plt.scatter(outliers[:, 0], outliers[:, 1], marker='o', color='red', picker=True)    \n",
    "\n",
    "x = np.linspace(-1, 1, 201)\n",
    "y = [pow(i, 9) for i in x]\n",
    "plt.plot(x, y)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.1, 1.1])\n",
    "axes.set_ylim([-1.1, 1.1])\n",
    "\n",
    "plt.grid(color='grey', linestyle='--', linewidth=.5)\n",
    "\n",
    "plt.savefig(str(tol) + '/' + str(trimming) + '-distance_from_curve.png')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking users: outliers or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAG = 'both'\n",
    "\n",
    "if (FLAG == 'outliers'):\n",
    "    array_being_analysed = outliers\n",
    "if (FLAG == 'normals'):\n",
    "    array_being_analysed = normal_points\n",
    "else:\n",
    "    array_being_analysed = np.concatenate((normal_points, outliers), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the hypotheses regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = []\n",
    "second = []\n",
    "third = []\n",
    "fourth = []\n",
    "fifth = []\n",
    "others = []\n",
    "\n",
    "plt.figure(figsize=(11, 7))\n",
    "plt.xlabel('User bias', fontsize=25)\n",
    "plt.ylabel('Score difference', fontsize=25)\n",
    "plt.title(\"Hypotheses\", fontsize=25)\n",
    "\n",
    "for i in range(0, len(array_being_analysed)):\n",
    "    d = array_being_analysed[i, 0]\n",
    "    sc = array_being_analysed[i, 1]\n",
    "\n",
    "    if (d >= 0.5 and sc >= 0.25):\n",
    "        first.append([d, sc])\n",
    "    elif (d >= 0.5 and sc > -0.25 and sc < 0.25):\n",
    "        second.append([d, sc])\n",
    "    elif (d > -0.5 and d < 0.5 and sc > -0.25 and sc < 0.25):\n",
    "        third.append([d, sc])\n",
    "    elif (d <= -0.5 and sc > -0.25 and sc < 0.25):\n",
    "        fourth.append([d, sc])\n",
    "    elif (d <= -0.5 and sc <= -0.25):\n",
    "        fifth.append([d, sc])\n",
    "    else:\n",
    "        others.append([d, sc])\n",
    "\n",
    "first = np.asarray(first)\n",
    "second = np.asarray(second)\n",
    "third = np.asarray(third)\n",
    "fourth = np.asarray(fourth)\n",
    "fifth = np.asarray(fifth)\n",
    "others = np.asarray(others)\n",
    "\n",
    "plt.scatter(first[:, 0], first[:, 1], marker='o', color='blue', s=250, picker=True)\n",
    "plt.scatter(second[:, 0], second[:, 1], marker='o', color='black', s=250, picker=True)\n",
    "plt.scatter(third[:, 0], third[:, 1], marker='o', color='black', s=250, picker=True)\n",
    "plt.scatter(fourth[:, 0], fourth[:, 1], marker='o', color='black', s=250, picker=True)\n",
    "plt.scatter(fifth[:, 0], fifth[:, 1], marker='o', color='orange', s=250, picker=True)\n",
    "plt.scatter(others[:, 0], others[:, 1], marker='o', color='grey', s=250, picker=True)\n",
    "\n",
    "x = np.linspace(-1, 1, 201)\n",
    "y = [pow(i, 9) for i in x]\n",
    "plt.plot(x, y)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.1, 1.1])\n",
    "axes.set_ylim([-1.1, 1.1])\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "plt.grid(color='grey', linestyle='--', linewidth=.5)\n",
    "\n",
    "plt.savefig(str(tol) + '/' + str(trimming) + '-hypotheses.png', bbox_inches='tight')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ip_addrs = check_ip_addresses(array_being_analysed, df)\n",
    "\n",
    "ips_in_common = []\n",
    "\n",
    "for i in range(0, len(ip_addrs)):\n",
    "    user_1_ips = ip_addrs[i][0]\n",
    "    user_2_ips = ip_addrs[i][1]\n",
    "    count_1 = ip_addrs[i][2]\n",
    "    count_2 = ip_addrs[i][3]\n",
    "    amount = 0\n",
    "    for j in range(0, len(user_1_ips)):\n",
    "        for k in range(0, len(user_2_ips)):\n",
    "            if ((user_1_ips[j] == user_2_ips[k]) == True):\n",
    "                amount = amount + min(count_1[j], count_2[k])\n",
    "    #print('-'*100)\n",
    "    #print('User 1 (', array_being_analysed[i][2], ',', len(user_1_ips), '): ', user_1_ips, count_1)\n",
    "    #print('User 2 (', array_being_analysed[i][3], ',', len(user_2_ips), '): ', user_2_ips, count_2)\n",
    "    #print('\\nCorrelation: ', amount, np.intersect1d(user_1_ips, user_2_ips, assume_unique=True))\n",
    "    ips_in_common.append(amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot checked users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.xlabel('User bias', fontsize=25)\n",
    "plt.ylabel('Course final score difference between users', fontsize=25)\n",
    "plt.title(\"Exercises with the same IP\", fontsize=25)\n",
    "\n",
    "scatter = plt.scatter(array_being_analysed[:, 0], array_being_analysed[:, 1], edgecolors = 'black', c=ips_in_common, cmap='binary')\n",
    "\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "#for i, txt in enumerate(ips_in_common):\n",
    "    #plt.annotate((txt, (array_being_analysed[i, 2], array_being_analysed[i, 3])), (array_being_analysed[i, 0], array_being_analysed[i, 1]))\n",
    "\n",
    "x = np.linspace(-1, 1, 201)\n",
    "y = [pow(i, 9) for i in x]\n",
    "plt.plot(x, y)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.1, 1.1])\n",
    "axes.set_ylim([-1.1, 1.1])\n",
    "plt.xticks(fontsize=20)\n",
    "\n",
    "plt.grid(color='grey', linestyle='--', linewidth=.5)\n",
    "\n",
    "plt.savefig(str(tol) + '/' + str(trimming) + '-exercises_same_ip-' + FLAG + '.png')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the material usage index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By counting the number of events a user has, it can measure how many of them were directed towards reading the materials. This is done by dividing the count of all the events by the count of the events where the user tried to solve an exercise. However, since a user can try the same exercise multiple times, it was decided that if the user tried the exercise at least one time (be it right or wrong), then it would count as a material usage towards that exercise and that's it, no additional tries for that exercise would be considered as material usage.\n",
    "\n",
    "Therefore, the formula is: number_of_material_usages / (number_of_material_usages + number_of_exercises_tried).\n",
    "\n",
    "The results mean: \n",
    "0 -> purely exercise tryouts (fake account); \n",
    "\n",
    "0,5 -> equal number of exercise tryouts and material reviews (legit user) and; \n",
    "\n",
    "1 -> purely material reviews (probably a professor or a material thief)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "material_usage = check_material_usage(array_being_analysed, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.xlabel('User bias', fontsize=25)\n",
    "plt.ylabel('Score difference', fontsize=25)\n",
    "plt.title(\"mMIR\", fontsize=25)\n",
    "\n",
    "material_usage_str = []\n",
    "\n",
    "for i in range(0, len(material_usage)):\n",
    "    material_usage_str.append(str(min(round(material_usage[i, 0], 2), round(material_usage[i, 1], 2))))\n",
    "\n",
    "c_intensities = []\n",
    "\n",
    "for i in range(0, len(array_being_analysed)):\n",
    "    c_intensities.append(min(material_usage[i, 0], material_usage[i, 1]))\n",
    "    \n",
    "scatter = plt.scatter(array_being_analysed[:, 0], array_being_analysed[:, 1], s=250, edgecolors = 'black', c=c_intensities, cmap='binary')\n",
    "\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "material_usage_dump = []\n",
    "\n",
    "for i, txt in enumerate(material_usage_str):\n",
    "    #plt.annotate((txt, (array_being_analysed[i, 2], array_being_analysed[i, 3])), (array_being_analysed[i, 0], array_being_analysed[i, 1]))\n",
    "    material_usage_dump.append('User: ' + str(array_being_analysed[i, 2]) + ' User: ' + str(array_being_analysed[i, 3]) + ' ' + txt)\n",
    "\n",
    "with open('material_usage_dump-' + FLAG + '.txt', 'w') as filehandle:\n",
    "    json.dump(material_usage_dump, filehandle)\n",
    "\n",
    "x = np.linspace(-1, 1, 201)\n",
    "y = [pow(i, 9) for i in x]\n",
    "plt.plot(x, y)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.1, 1.1])\n",
    "axes.set_ylim([-1.1, 1.1])\n",
    "plt.xticks(fontsize=20)\n",
    "\n",
    "plt.grid(color='grey', linestyle='--', linewidth=.5)\n",
    "\n",
    "plt.savefig(str(tol) + '/' + str(trimming) + '-material_usage_index-' + FLAG + '.png', bbox_inches='tight')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort material usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "material_usage_sorted = []\n",
    "material_usage_both = []\n",
    "material_usage_both.append(check_material_usage(normal_points, df))\n",
    "material_usage_both.append(check_material_usage(outliers, df))  \n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < len(user_pairs_copy):\n",
    "    user_1 = int(user_pairs_copy[i][0])\n",
    "    user_2 = int(user_pairs_copy[i][1])\n",
    "    found = False\n",
    "\n",
    "    for j in range(0, len(normal_points)):\n",
    "        if (user_1 == normal_points[j][2] and user_2 == normal_points[j][3]):\n",
    "            material_usage_sorted.append(material_usage_both[0][j])\n",
    "            found = True\n",
    "            i = i + 1\n",
    "            break\n",
    "\n",
    "    if (not found):   \n",
    "        for k in range(0, len(outliers)):\n",
    "            if (user_1 == outliers[k][2] and user_2 == outliers[k][3]):\n",
    "                material_usage_sorted.append(material_usage_both[1][k])\n",
    "                i = i + 1\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = []\n",
    "\n",
    "for i, txt in enumerate(analysing_data):\n",
    "    temp.append([analysing_data[i, 0], analysing_data[i, 1], user_pairs_copy[i][0], user_pairs_copy[i][1]])\n",
    "\n",
    "ip_addrs_temp = check_ip_addresses(temp, df)\n",
    "\n",
    "ips_in_common_temp = []\n",
    "\n",
    "for i in range(0, len(ip_addrs_temp)):\n",
    "    user_1_ips = ip_addrs_temp[i][0]\n",
    "    user_2_ips = ip_addrs_temp[i][1]\n",
    "    amount = 0\n",
    "    for j in range(0, len(user_1_ips)):\n",
    "        for k in range(0, len(user_2_ips)):\n",
    "            if (user_1_ips[j] == user_2_ips[k]):\n",
    "                amount = amount + 1\n",
    "    #print('Total:', len(user_1_ips), len(user_2_ips), amount)\n",
    "    #print('-'*100)\n",
    "    #print('User 1 (', array_being_analysed[i][2], ',', len(user_1_ips), '): ', user_1_ips, count_1)\n",
    "    #print('User 2 (', array_being_analysed[i][3], ',', len(user_2_ips), '): ', user_2_ips, count_2)\n",
    "    #print('\\nCorrelation: ', amount, np.intersect1d(user_1_ips, user_2_ips, assume_unique=True))\n",
    "    total = len(user_1_ips) + len(user_2_ips)\n",
    "    ips_in_common_temp.append(amount / (total - amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uip = np.asarray(user_interaction_percentages)\n",
    "mu = np.asarray(material_usage_sorted)\n",
    "\n",
    "CC = uip[:, 0]\n",
    "XC = uip[:, 1]\n",
    "XX = uip[:, 2]\n",
    "MIR = [min(pair[0], pair[1]) for pair in mu]\n",
    "ScoreDif = user_score_difference[:, 0]\n",
    "#IPs = np.asarray([i / j  for i, j in zip(ips_in_common_temp, total_exercises_under_tol)])\n",
    "IPs = np.asarray(ips_in_common_temp)\n",
    "UB = analysing_data[:, 0]\n",
    "#TE = total_exercises_under_tol\n",
    "\n",
    "ix = analysing_data[:, 0]<0\n",
    "\n",
    "UB[ix] = -UB[ix]\n",
    "ScoreDif[ix] = -ScoreDif[ix]\n",
    "\n",
    "MIR = np.asarray(MIR)\n",
    "\n",
    "dict_var = {'CC':CC, 'XC':XC, 'User Bias':UB, 'Minimal MIR':MIR, 'Score Diff':ScoreDif, \n",
    "            'IPs in common':IPs}\n",
    "#dict_var = {'CC':CC, 'XX':XX, 'User Bias':UB, 'Minimal MIR':MIR, 'Score Diff':ScoreDif, \n",
    "#            'IPs in common':IPs}\n",
    "\n",
    "for key, value in dict_var.items():\n",
    "    print(key, len(value), value.shape)\n",
    "    \n",
    "x = pd.DataFrame.from_dict(dict_var)\n",
    "\n",
    "# Get column names first\n",
    "names = x.columns\n",
    "# Create the Scaler object\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "x.index = user_pairs_copy\n",
    "x=x[:-1]\n",
    "\n",
    "scaled_x = scaler.fit_transform(x)\n",
    "scaled_x = pd.DataFrame(scaled_x, columns=names)\n",
    "\n",
    "pd.set_option('display.max_rows', len(x))\n",
    "#x.drop(x.tail(1).index,inplace=True)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "\n",
    "kmeans.fit_predict(scaled_x)\n",
    "\n",
    "print(kmeans.labels_)\n",
    "print(kmeans.cluster_centers_)\n",
    "\n",
    "x['Labels'] = kmeans.labels_\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "harvester = len(kmeans.labels_[kmeans.labels_ == 0]) \n",
    "person = len(kmeans.labels_[kmeans.labels_ == 1]) \n",
    "\n",
    "print('In this dataset, there are', harvester, 'harvester type copies and', person, 'person type copies.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", len(x['Labels'][x['Labels'] == 1]), \"1's and\", len(x['Labels'][x['Labels'] == 0]), \"0's as labels.\")\n",
    "prop_1 = len(x['Labels'][x['Labels'] == 1]) / len(x['Labels'])\n",
    "prop_0 = len(x['Labels'][x['Labels'] == 0]) / len(x['Labels'])\n",
    "print(\"The proportion of 1's and 0's is, repectively:\", prop_1, prop_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class_weights = {}\n",
    "class_weights[0] = prop_0\n",
    "class_weights[1] = prop_1\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "NUM_CV = 10\n",
    "\n",
    "X = scaled_x\n",
    "y = x['Labels']\n",
    "\n",
    "C = np.linspace(-5, 11, 9)\n",
    "C = [pow(2, i) for i in C] #2^[-5, -3, ... , 10]\n",
    "gamma = np.linspace(-1, 15, 9)\n",
    "gamma = [pow(2, i) for i in gamma] #2^[-1, 1, ... , 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up possible values of parameters to optimize over\n",
    "          \n",
    "p_grid = {'C': C, 'gamma': gamma}\n",
    "    \n",
    "svc = svm.SVC(kernel = 'linear', class_weight=class_weights, probability=True)\n",
    "clf = GridSearchCV(svc, param_grid=p_grid, cv=NUM_CV, iid=False, refit=True)\n",
    "print(clf)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "if (os.path.isfile(\"results_dump.csv\")):\n",
    "    os.remove(\"results_dump.csv\")\n",
    "\n",
    "if (os.path.isfile(\"X_dump.csv\")):\n",
    "    os.remove(\"X_dump.csv\")\n",
    "    \n",
    "if (os.path.isfile(\"sv_dump.csv\")):\n",
    "    os.remove(\"sv_dump.csv\")\n",
    "    \n",
    "def my_scoring(estimator, X, y):\n",
    "    results = estimator.predict_proba(X)\n",
    "    svs = estimator.best_estimator_.support_vectors_\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    with open('results_dump.csv', 'a') as f:\n",
    "        df_results.to_csv(f, header=False, index=False)\n",
    "    \n",
    "    df_X = pd.DataFrame(X)\n",
    "    with open('X_dump.csv', 'a') as f:\n",
    "        df_X.to_csv(f, header=False, index=False)\n",
    "        \n",
    "    df_sv = pd.DataFrame(svs)\n",
    "    with open('sv_dump.csv', 'a') as f:\n",
    "        df_sv.to_csv(f, header=False, index=False)\n",
    "    \n",
    "    #plt.figure()\n",
    "    #plt.hist(results[:, 0], bins=len(results))\n",
    "    #plt.show()\n",
    "    \n",
    "    return estimator.best_score_\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=NUM_CV, scoring=my_scoring)\n",
    "\n",
    "print(\"This model can differentiate harvesters (0) from collaborators (1) with an accuracy and standard \\\n",
    "deviation of, respectively: %0.3f (+/- %0.3f)\" % (scores.mean() * 100, scores.std() * 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv('results_dump.csv', index_col=0, header=None)\n",
    "df_results.fillna('', inplace=True)\n",
    "\n",
    "df_X = pd.read_csv('X_dump.csv', index_col=0, header=None)\n",
    "df_X.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(df_results), np.shape(df_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes = pd.concat([df_results.reset_index(), df_X.reset_index()], axis=1)\n",
    "df_classes.columns = ['0', '1', 'CC', 'XC', 'User Bias', 'Minimal MIR', 'Score Diff', 'IPs in common']\n",
    "df_classes['Clustering labels'] = kmeans.labels_\n",
    "#df_classes = df_classes.sort_values(by =['CC', 'XC', 'User Bias', 'Minimal MIR', 'Score Diff', 'IPs in common'])\n",
    "scaled_x.index = x.index\n",
    "#scaled_x = scaled_x.sort_values(by =['CC', 'XC', 'User Bias', 'Minimal MIR', 'Score Diff', 'IPs in common'])\n",
    "df_classes.index = scaled_x.index\n",
    "indexes = x.index\n",
    "indexes = sorted(indexes,key=lambda x: x[0])\n",
    "df_classes = df_classes.reindex(index=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for prob in df_classes['0']:\n",
    "    if (prob > 0.50):\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1) \n",
    "\n",
    "df_classes['Classification labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users = []\n",
    "\n",
    "for i in range(len(indexes)):\n",
    "    for user in indexes[i]:\n",
    "        if user not in unique_users:\n",
    "            unique_users.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_per_user = []\n",
    "\n",
    "for user in unique_users:\n",
    "    labels = []\n",
    "    for i in range(len(indexes)):\n",
    "        if (user == indexes[i][0]):\n",
    "            labels.append(df_classes[\"Clustering labels\"][i])\n",
    "        elif (user == indexes[i][1]):\n",
    "            labels.append(df_classes[\"Clustering labels\"][i])\n",
    "    labels_per_user.append(labels)\n",
    "            \n",
    "shifting_count = [np.unique(j, return_counts=True) for j in labels_per_user]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifting users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user, shifts in zip(unique_users, shifting_count):\n",
    "    #print(shifts[1])\n",
    "    if len(shifts[1]) > 1:\n",
    "        print(user, len(shifts[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
